---
title: 'Prueba Machine Learning : Ramon Yañez, Patricio Zapata'
date: "24-10-2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("rattle") 
#library(rattle.data)
library(rpart)
library(rattle)
library(rpart.plot)
library(randomForest)
library(e1071)
library(caret)
library(factoextra)
library(NbClust)
library(cluster)
library(dplyr)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)


```






<br />
<br />

### PREGUNTA 1 [0.75pt]
A veces, un algoritmo de machine learning no logra modelar bien los datos de entrenamiento ni generalizarse a nuevas observaciones. Elija la opción que mejor caracteriza a esta situación y justifique su elección. <br /> <br />
(a) Error de clasificación bajo.<br />
(b) Underfitting.<br />
(c) La varianza del error de estimación es pequeña.<br />
(d) Overfitting.<br />


**Respuesta: (b) Underfitting**<br />
Justificación: Tenemos underfitting cuando los datos son pocos y no se puede generalizar el conocimiento.
<br />



###  PREGUNTA 2 [0.75pt]
Hacer un árbol de decisión más profundo asegurará un mejor ajuste,
pero probablemente reducirá la exactitud (accuracy) de la validación cruzada. Justifique.
(a) Verdadero.
(b) Falso


**Respuesta: Verdadero** <br />
Justificación: Al tener mas profundidad, el ajuste con la data de entrenamiento calza 
plenamente pero cuando queremos generalizar fallará asi mismo como hacer una 
predicción con otra data o validación cruzada 
<br />


### PREGUNTA 3 

**Creacion Data frame**
```{r }

mi_df <- data.frame(
  "Edad" = c("18-40", "18-40", "18-40", "18-40", "18-40", "18-40", "18-40", "41-60", "41-60" , "41-60" , "41-60" , "41-60" , "41-60" , "41-60" , ">60" , ">60", ">60", ">60", ">60", ">60", ">60" ), 
  "Combo" = c("Fiesta", "Supremo", "Supremo", "Fiesta", "Supremo", "Mediano", "Supremo", "Mediano", "Mediano" , "Fiesta" , "Supremo" , "Fiesta" , "Supremo" , "Supremo" , "Fiesta" , "Mediano", "Mediano", "Fiesta", "Fiesta", "Supremo", "Supremo" ) 
)


glimpse(mi_df)
new <-  table(mi_df)
new

```
**(i) [0.25pt] Convierta el conjunto de datos en una tabla de frecuencia.**
```{r }

datos <- prop.table(new)
datos

```
**(ii) [0.25pt] Obtenga las probabilidades marginales por rango de Edad y Combo.**
```{r }

mayor_60 <-  sum(datos[1,])
mayor_60

entre_18_40  <-  sum(datos[2,])
entre_18_40

entre_41_60  <- sum(datos[3,])
entre_41_60


Fiesta  <- sum(datos[,1])
Fiesta

Mediano  <- sum(datos[,2])
Mediano

Supremo  <- sum(datos[,3])
Supremo
```
**(iii) [1.00pt] Calcule la probabilidad de recomendar cada Combo dadoel rango de Edad, es decir, la probabilidad posterior de recomendación.**
```{r }



# p(fiesta | mayor a 60 ) 
#    A           B
# = 
# ( p(mayor_60 | Fiesta) * P(fiesta) )  / P(mayor_60)
mayor_60_Fiesta <- datos[1,1]
(mayor_60_Fiesta * Fiesta) / mayor_60


# p(mediano | mayor a 60 ) 
#    A           B
# = 
# ( p(mayor_60 | Mediano) * P(Mediano) )  / P(mayor_60)
mayor_60_Mediano <- datos[1,2]
(mayor_60_Mediano * Mediano) / mayor_60


# p(supremo | mayor a 60 ) 
#    A           B
# = 
# ( p(mayor_60 | supremo) * P(supremo) )  / P(mayor_60)
mayor_60_Supremo <- datos[1,3]
(mayor_60_Supremo * Supremo) / mayor_60



# p(fiesta | 18_40 ) 
#    A           B
# = 
# ( p(18_40 | Fiesta) * P(fiesta) )  / P(18_40)
entre_18_40_Fiesta <- datos[2,1]
(entre_18_40_Fiesta * Fiesta) / entre_18_40


# p(mediano | 18_40 ) 
#    A           B
# = 
# ( p(18_40 | mediano) * P(mediano) )  / P(18_40)
entre_18_40_mediano <- datos[2,2]
(entre_18_40_mediano * Mediano) / entre_18_40


# p(supremo | 18_40 ) 
#    A           B
# = 
# ( p(18_40 | supremo) * P(supremo) )  / P(18_40)
entre_18_40_supremo <- datos[2,3]
(entre_18_40_supremo * Supremo) / entre_18_40




# p(fiesta | 41_60 ) 
#    A           B
# = 
# ( p(41_60 | Fiesta) * P(fiesta) )  / P(41_60)
entre_41_60_Fiesta <- datos[3,1]
(entre_41_60_Fiesta * Fiesta) / entre_41_60


# p(mediano | 41_60 ) 
#    A           B
# = 
# ( p(41_60 | mediano) * P(mediano) )  / P(41_60)
entre_41_60_mediano <- datos[3,2]
(entre_41_60_mediano * Mediano) / entre_41_60


# p(supremo | 41_60 ) 
#    A           B
# = 
# ( p(41_60 | supremo) * P(supremo) )  / P(41_60)
entre_41_60_supremo <- datos[3,3]
(entre_41_60_supremo * Supremo) / entre_41_60

```
**(iv) [0.25pt] Si un cliente de 30 años (es decir, rango de edad: “18-40”) se conecta al sistema, ¿qué Combo recomendaría el algoritmo?**
<br />

  Le recomendaría el combo supremo ya que nos da un 24% de probabilidad que lo consuma


<br />

### PREGUNTA 4 


La base de datos wine (disponible en el paquete rattle.data) contiene
información de 178 vinos. Se desea clasificar el tipo de vino (Type=1,2,3) a partir de 13
covariables disponibles. Aquí hay más detalles sobre cada variable.
Cargue la base de datos y desarrolle los siguientes ítems:

**(i) [0.25pt] Estandarice las variables continuas.**
```{r }
data <- wine
glimpse(data)
summary(data)

data[,-1] <- as.data.frame(scale(data[,-1]))
data[,-1]


```




**(ii) [0.25pt] Divida la base de datos en 70% para entrenamiento y 30% para prueba (Utilice una semilla para que el resultado sea replicable).**
```{r }

set.seed(2020)
posTraining <- sample(1:nrow(data), 0.7*nrow(data))
posTraining
data_training <- data[posTraining, ]
data_training
data_test <- data[-posTraining, ]
data_test
nrow(data_training) + nrow(data_test) # Corroboramos 178 filas


```



**(iii) [0.25pt] Implemente los algoritmos de Random Forest (20 árboles y 4 variables) y Support Vector Machine (kernel radial) utilizando todas las variables.**
```{r }

#set.seed(2020)
# randomForest
modelo.rf <- randomForest(Type ~ ., data=data_training, 
                         ntree=20, mtry=4, replace=TRUE, importance=T)

varImpPlot(modelo.rf)



# set.seed(2020)
# SVM
modelo.svm <- svm(Type ~ ., data=data_training, kernel="radial")
modelo.svm 
```




**(iv) [0.25pt] ¿Cuál es el porcentaje de clasificaciones correctas para cada algoritmo utilizando la base de entrenamiento? **

### RANDOM FOREST Predicciones
```{r }


# Valores predictivos
predval.rf1 <- predict(modelo.rf, data_training)
# Matriz de confusión
table(data_training$Type, predval.rf1)
# % de clasificación correcta
mean(data_training$Type == predval.rf1)

# --> El % de predicción respecto a la data de entrenamiento con modelo Random Forest es de 100%
```
### SVM Predicciones
```{r }


predval.svm <- predict(modelo.svm, data_training)
# Matriz de confusión
table(data_training$Type, predval.svm)
# % de clasificación correcta
mean(data_training$Type == predval.svm)

# --> El % de predicción respecto a la data de prueba con modelo Support Vector Machine es de 
# 100% (es claramente un sobreajuste puesto que estamos usando la misma base de datos de entrenamiento).
```





**(v) [0.25pt] Aplique una validación cruzada para ambos algoritmos usando 30 folds (number) y 70% para la submuestra de entrenamiento (p). Basado en la validación cruzada, elija uno de los métodos como el más apropiado.**

### CV con Random Forest
```{r }
set.seed(2020)

# 
( cv_rf <- train(Type ~., data=data_training, method="rf", 
                 trControl=trainControl(method="cv", number=30, p=0.7), 
                 tuneGrid=expand.grid(.mtry=3)) )
# La validacion cruzada da un accuracy de 0.993 para Random Forest
```
# CV con SVM
```{r }

set.seed(2020)
( cv.svm <- train(Type ~., data=data_training, method="svmRadial",
                  trControl=trainControl(method="cv", number=30, p=0.7)) )
# Con sigma = 0.0830777 and C = 1, me da un accuracy de 0.991




```
Se elige algoritmo Random Forest, ya que entrega un accuracy superior al SVM.





**(vi) [0.25pt] Aplique ambos modelos ajustados a la base de prueba y concluya si el modelo elegido en el ítem anterior predice mejor.**

### Predicciones con BD de prueba para RANDOM FOREST
```{r }
# Valores predictivos
predval.rf2 <- predict(modelo.rf, data_test)
# Matriz de confusión
table(data_test$Type, predval.rf2)
# % de clasificación correcta
mean(data_test$Type == predval.rf2)

# El % de predicción respecto a la data de prueba con modelo Random Forest es de 96%

# entrega el algoritmo de SVM.
```
### Predicciones con BD de prueba para SVM
```{r }
# Ajustamos el modelo con parametros de Validacion Cruzada.
modelo.svm.ajustado <- svm(Type ~ ., data=data_training, kernel="radial",C=1, sigma=0.0830777 )


# SVM Predicciones
predval.svm2 <- predict(modelo.svm.ajustado, data_test)
# Matriz de confusión
table(data_test$Type, predval.svm2)
# % de clasificación correcta
mean(data_test$Type == predval.svm2)

# El % de predicción respecto a la data de prueba con modelo Support Vector Machine es de 98% 



```
####    En la pregunta 5, basándonos en accuracy, el Random Forest fue mejor predictor. Pero en la
####    práctica haciendo predicciones con base de datos de test, la mejor matriz de confusion la 
####    entrega el algoritmo de SVM.




### PREGUNTA 5
**(i) [0.25pt] Aplique el Clustering Jerárquico con k=3 y linkage “ward.D”. Visualice el resultado final con un gráfico dendrograma.**

```{r }

data_5 <-  data[,-1]

# Matriz de distancias euclídeas
d <- dist(data_5, method="euclidean")

# Dendrograma estándar
fviz_dend(hcut(data_5, k=3, hc_method="ward.D"), rect=TRUE)
model_hcut <- hcut(data_5, k = 3, hc_method = "ward.D")

```



**(ii) [0.25pt] Haga un PCA con 2 componentes principales y guarde los scores de ambos componentes como nuevas variables.**

```{r }

# PCA
model_pca <- prcomp(data_5)
model_pca$rotation[,1]

## Seleccion de componentes
# Escojo 2 componentes principales
fviz_eig(model_pca) 
summary(model_pca) 


## Grafico de contribucicion  de las variables en PC1 y PC2
fviz_pca_var(model_pca)
fviz_pca_ind(model_pca)


# contribucicion de cada variable en las componentes principales
fviz_contrib(model_pca, choice = "var" , axes = 1)
fviz_contrib(model_pca, choice = "var" , axes = 2)



# Obtener componentes principales para cada observacion
componentes_principales <- as_tibble(model_pca$x[,1:2]) 
```




**(iii) [0.25pt] Utilice los scores del ítem anterior para graficar los datos y pinte cada punto según la agrupación calculada con el clustering jerárquico del ítem (i).**

<br />
Grafico de visualizacion de clusters
```{r }


grafico_cluster <- fviz_cluster(model_hcut, data = data_5)
grafico_cluster
```


**(iv) [0.50pt] Haga el mismo gráfico del ítem anterior pero ahora pinte los puntos según el tipo de vino (variable Type). Compare este gráfico con el del ítem (iii) y concluya si la agrupación del clustering jerárquico podría ser un buen clasificador de vinos..**

<br />
Grafico de visualizacion de clusters
```{r }

componentes_principales['type'] = data$Type

graficos_componente_type <- ggplot(data = componentes_principales, aes(x=PC1, y=PC2, color=type )) +
  geom_point()



# comparacion del grafico_cluster y graficos_componente_type 

grid.arrange(grafico_cluster,graficos_componente_type,  ncol=2)



```
se puede apreciar que ambos graficos son muy similares, pero comparando las clasificaciones de tipo vs  las clasificaciones de cluster se puede  ver una pequeñas diferencias.
se podria decir que el cluster de vinos clasifica bien

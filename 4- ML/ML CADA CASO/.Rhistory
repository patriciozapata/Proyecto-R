return( (x-min(x))/(max(x)-min(x)) )
}
Xnew.fa <- apply(Xnew.fa,2,normalize)
summary(Xnew.fa)
Xnew.fa <- model.fa$scores[,1:2]
summary(Xnew.fa)
normalize <- function(x){
return( (x-min(x))/(max(x)-min(x)) )
}
normalize
Xnew.fa <- apply(Xnew.fa,2,normalize)
summary(Xnew.fa)
data <- scale(pregunta_5)
data
hc4 <- diana(pregunta_5)
load <- model.fa$loadings[,1:2]
load
fviz_cluster(list(data = data, cluster = Xnew.fa))
require(igraph)
fviz_dend(hcut(pregunta_5, k=3), type="phylogenic", repel=TRUE)
fviz_dend(hcut(pregunta_5, k=3, hc_method="ward.D2"), rect=TRUE)
set.seed(2020)
k.nbclust <- NbClust(pregunta_5, distance="euclidean", max.nc=15, method="ward.D2", index="alllong")
fviz_nbclust(k.nbclust) + theme_minimal()
aggregate(pregunta_5, by=list(hcut(pregunta_5,k=3)$cluster), median)
require(psy)
scree.plot(cor(pregunta_5))
model.fa <- factanal(pregunta_5, factors=3, scores=c("regression"))
print(model.fa)
load <- model.fa$loadings[,1:2]
plot(load, type="n", xlim=c(-0.1,1))
text(load, labels=names(pregunta_5), cex=0.7)
Xnew.fa <- model.fa$scores[,1:2]
summary(Xnew.fa)
normalize <- function(x){
return( (x-min(x))/(max(x)-min(x)) )
}
Xnew.fa <- apply(Xnew.fa,2,normalize)
summary(Xnew.fa)
clust.jer <- hclust(dist(Xnew.fa))
plot(clust.jer, labels = pregunta_5$labs, cex = 0.4,
main = "Clust. jerárquico sobre componentes principales")
pregunta_5
model.fa <- factanal(pregunta_5, factors=3, scores=c("regression"))
print(model.fa)
load <- model.fa$loadings[,1:2]
plot(load, type="n", xlim=c(-0.1,1))
text(load, labels=names(pregunta_5), cex=0.7)
km_res <- kmeans(load, centers = 2, nstart = 20)
fviz_cluster(km_res, load, ellipse.type = "norm")
load <- model.fa$loadings[,1:2]
load
Xnew.fa <- model.fa$scores[,1:2]
summary(Xnew.fa)
library(rattle.data)
library(dplyr)
data <- wine
glimpse(data)
summary(data)
head(data)
data[,-1] <- as.data.frame(scale(data[,-1]))
data[,-1]
set.seed(69)
posTraining <- sample(1:nrow(data), 0.8*nrow(data))
data_training <- data[posTraining, ]
data_test <- data[-posTraining, ]
nrow(data_training) + nrow(data_test) # Corroboramos 178 filas
library(rpart)
library(rattle)
library(rpart.plot)
modelo.dt <- rpart(Type ~ ., data=data_training)
x11()
fancyRpartPlot(modelo.dt)
rpart.rules(modelo.dt, cover=TRUE)
library(e1071)
modelo.svm <- svm(Type ~ ., data=data_training, kernel="radial")
pred <- predict(modelo.dt, data_training)
predval.dt <- as.numeric(colnames(pred)[apply(pred,1,which.max)])
mean(data_training$Type == predval.dt)
# SVM Predicciones
predval.svm <- predict(modelo.svm, data_training)
# Matriz de confusión
table(data_training$Type, predval.svm)
# % de clasificación correcta
mean(data_training$Type == predval.svm)
library(caret)
set.seed(69)
( cv.dt <- train(Type ~., data=data_training, method="rpart",
trControl=trainControl(method="cv", number=30, p=0.7)) )
( cv.svm <- train(Type ~., data=data_training, method="svmRadial",
trControl=trainControl(method="cv", number=30, p=0.7)) )
modelo.dt.ajustado <- rpart(Type ~ ., data=data_training, cp=0) ## COMO MIERDA HACEMOS EL MODELO AJUSTADO??? Internet me dijo que carepalo el CP=0...
# DT predicciones
pred <- predict(modelo.dt.ajustado, data_test)
predval.dt <- as.numeric(colnames(pred)[apply(pred,1,which.max)])
mean(data_test$Type == predval.dt)
modelo.svm.ajustado <- svm(Type ~ ., data=data_training, kernel="radial",C=0.25, sigma=0.06735516 )
# SVM Predicciones
predval.svm <- predict(modelo.svm.ajustado, data_test)
# Matriz de confusión
table(data_test$Type, predval.svm)
# % de clasificación correcta
mean(data_test$Type == predval.svm)
groupby
ind <- sample(2,nrow(groupby$Edad), replace = TRUE, prob = c(0.7,0.3) ) #60% entrenamiento y 40% test
trainData<- groupby[ind==1,]
testData<- groupby[ind==2,]
library(e1071)
library(naivebayes)
library(caret)
library(C50)
install.packages('naivebayes')
install.packages('C50')
library(e1071)
#install.packages('naivebayes')
library(naivebayes)
library(caret)
#install.packages('C50')
library(C50)
ind <- sample(2,nrow(groupby$Edad), replace = TRUE, prob = c(0.7,0.3) ) #60% entrenamiento y 40% test
trainData<- groupby[ind==1,]
testData<- groupby[ind==2,]
ind <- sample(2,nrow(groupby$Edad), replace = TRUE, prob = c(0.7,0.3) ) #60% entrenamiento y 40% test
library(knitr)
library(ggplot2)
library(tidyr)
library(caret)
library(ROCR)
install.packages('ROCR')
install.packages("ROCR")
library(ROCR)
Index = sample(1:nrow(groupby), size = round(0.6*nrow(groupby)), replace=FALSE)
train = data[Index ,]
test = data[-Index ,]
Index
train
Index = sample(1:nrow(groupby), size = round(0.6*nrow(groupby)), replace=FALSE)
train = groupby[Index ,]
test = groupby[-Index ,]
train
test
datos
Index = sample(1:nrow(datos), size = round(0.6*nrow(groupby)), replace=FALSE)
train = datos[Index ,]
test = datos[-Index ,]
Index
train
test
rm(Index)
rm(Index)
Index = sample(1:nrow(datos), size = round(0.6*nrow(groupby)), replace=FALSE)
train = datos[Index ,]
test = datos[-Index ,]
Index
rm(Index)
NBClassifier = naiveBayes(Win.Loss ~., data = train)
NBClassifier
library(e1071)
NBClassifier = naiveBayes(datos$Edad~., data = train)
NBClassifier
NBClassifier = naiveBayes(datos$Edad~., data = train)
datos
NBClassifier = naiveBayes(factor(datos$Edad)~datos$[-1], data = train)
NBClassifier = naiveBayes(factor(datos$Edad)~datos, data = train)
datos
NBClassifier = naiveBayes(factor(datos$Edad)~., data = train)
new <-  datos[-1]
NBClassifier = naiveBayes(factor(datos$Edad)~new, data = train)
diagnosis
library(tidyverse);library(caret);library(rpart)
library(rpart.plot);library(rattle);library(MLmetrics)
library(readr)
Cancer <- read_csv("Cancer.csv")
## Quitamos la columna id y transformamos diagnosis
Cancer <- Cancer %>%
select(-id) %>%
mutate(diagnosis = factor(ifelse(diagnosis == "M", "0", "1"))) %>%
drop_na() %>%
rename("concavepointsmean" = `concave points_mean`)
view(Cancer)
## Observemos correlaciÃ³n entre variables
library(corrplot)
Cancer %>%
select(-diagnosis) %>%
cor() %>%
corrplot()
diagnosis
Cancer
Cancer
mi_df
datos <- mi_df[-1]
train <- slice_sample(datos, prop = 0.8)
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
groupby
datos
train
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
predval_NB <- predict(model_NB, train)
library(tidyverse);library(caret);library(rpart)
library(rpart.plot);library(rattle);library(MLmetrics)
library(readr)
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$diagnosis)
error_NB
ConfusionMatrix(predval_NB,train$diagnosis)
datos
mi_df
mi_df
datos
mi_df
datos <- mi_df[-1]
datos
datos <- mi_df[-1,-2]
datos
datos <- mi_df[-1:-2]
datos
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
mi_df
mi_df[1]
library(e1071)
model_NB <- naiveBayes(mi_df[1] ~., data=train)
id <- mi_df[1]
id <- factor(mi_df[1])
id
library(e1071)
model_NB <- naiveBayes(id ~., data=train)
test <- anti_join(datos,train)
test
train <- slice_sample(datos, prop = 0.8)
train
datos <- mi_df[-1:]
datos <- mi_df[-1]
datos
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
library(e1071)
model_NB <- naiveBayes(Combo ~., data=train)
model_NB
predval_NB <- predict(model_NB, train)
predval_NB
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$diagnosis)
ConfusionMatrix(predval_NB,train$diagnosis)
error_NB
ConfusionMatrix(predval_NB,train$diagnosis)
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$Combo)
ConfusionMatrix(predval_NB,train$Combo)
predval_NB
error_NB
ConfusionMatrix(predval_NB,train$Combo)
test <- anti_join(datos,train)
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
model_NB_cv <- train(Combo ~., data=train, method="nb",
trControl=trainControl(method="cv", number=5, p=0.8))
model_NB_cv
pred_NB <- predict(model_NB,test)
error_NB <- Precision(test$Combo, pred_NB, positive = 1)
F_score_NB <- F1_Score(test$Combo, pred_NB, positive = 1)
test <- anti_join(datos,train)
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
pred_NB <- predict(model_NB,test)
error_NB <- Precision(test$Combo, pred_NB, positive = 1)
F_score_NB <- F1_Score(test$Combo, pred_NB, positive = 1)
library(tidyverse);library(caret);library(rpart)
library(rpart.plot);library(rattle);library(MLmetrics)
library(readr)
Cancer <- read_csv("Cancer.csv")
## Quitamos la columna id y transformamos diagnosis
Cancer <- Cancer %>%
select(-id) %>%
mutate(diagnosis = factor(ifelse(diagnosis == "M", "0", "1"))) %>%
drop_na() %>%
rename("concavepointsmean" = `concave points_mean`)
view(Cancer)
## Observemos correlaciÃ³n entre variables
library(corrplot)
Cancer %>%
select(-diagnosis) %>%
cor() %>%
corrplot()
## Quitaremos todas las columnas worst y se
Cancer <- Cancer %>%
select(diagnosis, contains("mean"))
## Generamos base de datos de entrenamiento y testeo
set.seed(2020)
train <- slice_sample(Cancer, prop = 0.8)
test <- anti_join(Cancer,train)
# 5- Naive Bayes -------------------------------------------------------------
library(e1071)
model_NB <- naiveBayes(diagnosis ~., data=train)
## Calidad de ajuste
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$diagnosis)
ConfusionMatrix(predval_NB,train$diagnosis)
## ValidaciÃ³n cruzada
model_NB_cv <- train(diagnosis ~., data=train, method="nb",
trControl=trainControl(method="cv", number=5, p=0.8))
## ValidaciÃ³n
pred_NB <- predict(model_NB,test)
error_NB <- Precision(test$diagnosis, pred_NB, positive = 1)
F_score_NB <- F1_Score(test$diagnosis, pred_NB, positive = 1)
F_score_NB
Cancer <- Cancer %>%
select(-id) %>%
mutate(diagnosis = factor(ifelse(diagnosis == "M", "0", "1"))) %>%
drop_na() %>%
rename("concavepointsmean" = `concave points_mean`)
Cancer
library(corrplot)
Cancer %>%
select(-diagnosis) %>%
cor() %>%
corrplot()
Cancer
Cancer <- Cancer %>%
select(diagnosis, contains("mean"))
Cancer
Cancer <- Cancer %>%
select(-id) %>%
mutate(diagnosis = factor(ifelse(diagnosis == "M", "0", "1"))) %>%
drop_na() %>%
rename("concavepointsmean" = `concave points_mean`)
view(Cancer)
## Observemos correlaciÃ³n entre variables
library(corrplot)
Cancer %>%
select(-diagnosis) %>%
cor() %>%
corrplot()
Cancer
view(Cancer)
model_NB <- naiveBayes(diagnosis ~., data=train)
source('C:/Users/patri/OneDrive/Escritorio/ML/Ayudantía_script.R', echo=TRUE)
model_NB
train <- slice_sample(Cancer, prop = 0.8)
train
datos
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
datos
datos <- mi_df[-2]
datos
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
id <- factor(mi_df[1])
id
train <- slice_sample(datos, prop = 0.8)
train
model_NB <- naiveBayes(Edad  ~., data=train)
model_NB
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$Edad)
ConfusionMatrix(predval_NB,train$Edad)
predval_NB
error_NB
predval_NB
predval_NB <- predict(model_NB, train)
predval_NB
error_NB <- mean( predval_NB != train$diagnosis)
error_NB
library(tidyverse);library(caret);library(rpart)
library(rpart.plot);library(rattle);library(MLmetrics)
library(readr)
Cancer <- read_csv("Cancer.csv")
## Quitamos la columna id y transformamos diagnosis
Cancer <- Cancer %>%
select(-id) %>%
mutate(diagnosis = factor(ifelse(diagnosis == "M", "0", "1"))) %>%
drop_na() %>%
rename("concavepointsmean" = `concave points_mean`)
view(Cancer)
## Observemos correlaciÃ³n entre variables
library(corrplot)
Cancer %>%
select(-diagnosis) %>%
cor() %>%
corrplot()
## Quitaremos todas las columnas worst y se
Cancer <- Cancer %>%
select(diagnosis, contains("mean"))
## Generamos base de datos de entrenamiento y testeo
set.seed(2020)
train <- slice_sample(Cancer, prop = 0.8)
test <- anti_join(Cancer,train)
# 5- Naive Bayes -------------------------------------------------------------
library(e1071)
model_NB <- naiveBayes(diagnosis ~., data=train)
## Calidad de ajuste
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$diagnosis)
ConfusionMatrix(predval_NB,train$diagnosis)
predval_NB
model_NB <- naiveBayes(diagnosis ~., data=train)
model_NB
train
predval_NB <- predict(model_NB, train)
predval_NB
predval_NB <- predict(model_NB, train)
predval_NB
error_NB <- mean( predval_NB != train$Edad)
train <- slice_sample(datos, prop = 0.8)
train
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$Edad)
set.seed(2020)
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
id <- factor(mi_df[1])
# 5- Naive Bayes -------------------------------------------------------------
library(e1071)
model_NB <- naiveBayes(Edad  ~., data=train)
## Calidad de ajuste
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$Edad)
predval_NB
predval_NB <- predict(model_NB, train)
predval_NB
model_NB
library(e1071)
model_NB <- naiveBayes(Edad  ~., data=train)
## Calidad de ajuste
predval_NB <- predict(model_NB, train)
predval_NB
train <- slice_sample(datos, prop = 0.8)
test <- anti_join(datos,train)
train
test
library(e1071)
model_NB <- naiveBayes(factor(Edad)  ~., data=train)
model_NB
predval_NB <- predict(model_NB, train)
predval_NB
error_NB <- mean( predval_NB != train$Edad)
error_NB
ConfusionMatrix(predval_NB,train$Edad)
predval_NB
train$Edad
mi_df
datos
groupby
train
test
source('C:/Users/patri/OneDrive/Escritorio/ML/Script HCDS ML (TALLER).R', echo=TRUE)
model_NB <- naiveBayes(factor(Edad)  ~., data=train)
model_NB
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$Edad)
ConfusionMatrix(predval_NB,train$Edad)
train
test
error_NB <- mean( predval_NB != train$Edad)
error_NB
model_NB_cv <- train(Combo ~., data=train, method="nb",
trControl=trainControl(method="cv", number=5, p=0.8))
predval_NB <- predict(model_NB, train)
error_NB <- mean( predval_NB != train$Edad)
ConfusionMatrix(predval_NB,train$Edad)
model_NB <- naiveBayes(factor(Edad)  ~., data=train)
model_NB
predval_NB <- predict(model_NB, train)
predval_NB
error_NB <- mean( predval_NB !=  .)
error_NB <- mean( predval_NB != train)
predval_NB
error_NB <- mean( predval_NB != train)
train
prop.table(mi_df)
prop.table(mi_df)
mi_df <- mi_df %>%
mutate(Fiesta = as.numeric(ifelse(Combo == "Fiesta",1,0))) %>%
mutate(Supremo = as.numeric(ifelse(Combo == "Supremo",1,0))) %>%
mutate(Mediano = as.numeric(ifelse(Combo == "Mediano",1,0)))
prop.table(mi_df)
prop.table(mi_df)
mi_df
prop.table(mi_df[-1:-2])
mi_df <- mi_df %>%
mutate(Combo = as.numeric(ifelse(Combo == "Fiesta",1,ifelse(Combo == "Supremo",1,ifelse(Combo == "Mediano",1,0))))) %>%
mi_df
mi_df
mi_df <- mi_df %>%
mutate(Combo = as.numeric(ifelse(Combo == "Fiesta",1,ifelse(Combo == "Supremo",1,ifelse(Combo == "Mediano",1,0))))) %>%
mi_df
mi_df
mi_df <- mi_df %>%
mutate(Combo = as.numeric(ifelse(Combo == "Fiesta",1,ifelse(Combo == "Supremo",1,ifelse(Combo == "Mediano",1,0)))))
mi_df
mi_df <- mi_df %>%
mutate(Combo = as.numeric(ifelse(Combo == "Fiesta",1,ifelse(Combo == "Supremo",1,ifelse(Combo == "Mediano",1,0)))))
mi_df
mi_df <- mi_df %>%
mutate(Combo = as.numeric(ifelse(Combo == "Fiesta",1,ifelse(Combo == "Supremo",1,ifelse(Combo == "Mediano",1,0)))))
mi_df
prop.table(mi_df[-2])
prop.table(mi_df[-1:-2])
model_NB

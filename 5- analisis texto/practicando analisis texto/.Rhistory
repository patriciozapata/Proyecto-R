head(titulares_tecnologia)
# veamos cuáles son los bigramas más frecuentes -----
# importar una lista de stopwords
algunas_stopwords <- read_csv("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt")
mas_stopwords <- tibble(palabra = c("va", "hace", "hacer", "ser", "puede", "tiene", "dice"))
las_stopwords <- bind_rows(algunas_stopwords, mas_stopwords)
titulares_tecnologia %>%
unnest_tokens(input = titular,
output = palabra,
token = "ngrams",
n = 2) %>%
filter(!is.na(palabra)) %>%
count(palabra, sort = TRUE) %>%
separate(col = palabra,
into = c("palabra_1", "palabra_2"),
sep = " ") %>%
filter(!palabra_1 %in% las_stopwords$palabra) %>%
filter(!palabra_2 %in% las_stopwords$palabra)
# argumento de encoding fileEncoding / iconv() / str_conv()
noticias <- read_csv("http://bit.ly/500-noticias-tecnologia")
head(noticias)
View(noticias)
frecuencias_noticias <- noticias %>%
mutate(documento = 1:nrow(noticias), .before = fecha) %>%
unnest_tokens(input = texto_noticia,
output = palabra) %>%
anti_join(las_stopwords) %>%
count(documento, palabra, sort = TRUE)
frecuencias_noticias %>%
filter(palabra == "henry")
noticias_dtm <- frecuencias_noticias %>%
cast_dtm(documento, palabra, n)
noticias_lda <- LDA(noticias_dtm, k = 6, control = list(seed = 1234))
noticias_tema <- tidy(noticias_lda, matrix = "beta")
noticias_tema %>%
group_by(topic) %>%
slice_max(beta, n = 5) %>%
arrange(topic, -beta) %>%
ggplot(aes(y = term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free")
#https://vas3k.com/blog/machine_learning/
# anaklisis supervissado  https://vas3k.com/blog/machine_learning/https://smltar.com/
#  https://quanteda.io/ quiero entender el concepto de una palabra
algunas_stopwords <- read_csv("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt")
mas_stopwords <- tibble(palabra = c("va", "hace", "hacer", "ser", "puede", "tiene", "dice"))
las_stopwords <- bind_rows(algunas_stopwords, mas_stopwords)
titulares_tecnologia %>%
unnest_tokens(input = titular,
output = palabra,
token = "ngrams",
n = 2) %>%
filter(!is.na(palabra)) %>%
count(palabra, sort = TRUE) %>%
separate(col = palabra,
into = c("palabra_1", "palabra_2"),
sep = " ") %>%
filter(!palabra_1 %in% las_stopwords$palabra) %>%
filter(!palabra_2 %in% las_stopwords$palabra)
titulares_tecnologia %>%
unnest_tokens(input = titular,
output = palabra,
token = "ngrams",
n = 2) %>%
filter(!is.na(palabra)) %>%
count(palabra, sort = TRUE) %>%
separate(col = palabra,
into = c("palabra_1", "palabra_2"),
sep = " ") %>%
filter(!palabra_1 %in% las_stopwords$palabra) %>%
filter(!palabra_2 %in% las_stopwords$palabra)
frecuencias_2019 <- read.csv(file="http://bit.ly/frecuencias_2019",fileEncoding="utf-8")
palabras_2019 <- frecuencias_2019 %>%
anti_join(unas_stopwords) %>%
anti_join(mis_stopwords) %>%
filter(!str_detect(palabra,"chile"))
# graficamos..
palabras_2019 %>%
slice_head(n=15) %>%
ggplot(aes(y= reorder(palabra,n),n)) +
geom_col(fill = "#0057e7") +
geom_text(aes(label=n)) +
labs(y=NULL,
x="Frecuencia",
title = "Palabras mas frecuentes mensaje 2019",
subtitle = "Presidente Sebasti?n Pi?era",
caption = "Fuente: analisis propio a partir de documento BCN") +
theme_minimal()
titulares_tecnologia
head(titulares_tecnologia)
## Diplomado en Data Science 2020
## Módulo: Análisis de textos con R
## Prof: Riva Quiroga
## Script sesión 3 (10 de noviembre)
## Los paquetes que vamos a utilizar ----
library(rvest)
library(robotstxt)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(readr)
library(tidytext)
library(topicmodels)
library(ggplot2)
#install.packages("get_robotstxt")
library(get_robotstxt)
## Obtener los datos ----
#get_robotstxt("https://www.amazon.com")
get_robotstxt("https://www.latercera.com/etiqueta/tecnologia/page/1/")
enlace_p1 <- read_html("https://www.latercera.com/etiqueta/tecnologia/page/1/")
texto_titulares <- enlace_p1 %>%
html_nodes("h3") %>%
html_text(trim = TRUE) %>%
.[-1]
enlace_titulares <- enlace_p1 %>%
html_nodes("h3 > a") %>%
html_attr("href")
tibble(titular = texto_titulares,
enlace_noticia = enlace_titulares) %>%
mutate(enlace_noticia = paste0("https://www.latercera.com", enlace_noticia)) %>%
separate(col = titular,
into = c("seccion", "titular"),
sep = "  ",
extra = "merge")
## vamos a convertir este código en una función
obtener_titulares <- function(numero_pagina) {
Sys.sleep(5)
enlace <- paste0("https://www.latercera.com/etiqueta/tecnologia/page/", numero_pagina)
html <- read_html(enlace)
texto_titulares <- html %>%
html_nodes("h3") %>%
html_text(trim = TRUE) %>%
.[-1]
enlace_titulares <- html %>%
html_nodes("h3 > a") %>%
html_attr("href")
tibble(titular = texto_titulares,
enlace_noticia = enlace_titulares) %>%
mutate(enlace_noticia = paste0("https://www.latercera.com", enlace_noticia)) %>%
separate(col = titular,
into = c("seccion", "titular"),
sep = "  ",
extra = "merge")
}
# probar con un titular
obtener_titulares(17)
### identificar el número total de páginas (PENDIENTE)
# iterar
map(1:3, obtener_titulares)
map_df(1:3, obtener_titulares)
# Si tuviésemos tiempo, ejecutaríamos lo siguiente:
# titulares_tecnologia <- map_df(1:152, obtener_titulares)
titulares_tecnologia <- read_csv("https://bit.ly/titulares-tecnologia-latercera")
# read.csv(url, encoding = "UTF-8")
# read.csv("https://raw.githubusercontent.com/rivaquiroga/datos-clases/main/dipDS_2020/2020-11-10_titulares-tecnologia.csv?token=AHPXJMFJEKM3BPGY6Q5VZ4K7WQZQI",fileEncoding="utf-8") # por si tienen problemas con la codificación de caracteres
head(titulares_tecnologia)
# veamos cuáles son los bigramas más frecuentes -----
# importar una lista de stopwords
algunas_stopwords <- read_csv("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt")
mas_stopwords <- tibble(palabra = c("va", "hace", "hacer", "ser", "puede", "tiene", "dice"))
las_stopwords <- bind_rows(algunas_stopwords, mas_stopwords)
titulares_tecnologia %>%
unnest_tokens(input = titular,
output = palabra,
token = "ngrams",
n = 2) %>%
filter(!is.na(palabra)) %>%
count(palabra, sort = TRUE) %>%
separate(col = palabra,
into = c("palabra_1", "palabra_2"),
sep = " ") %>%
filter(!palabra_1 %in% las_stopwords$palabra) %>%
filter(!palabra_2 %in% las_stopwords$palabra)
# argumento de encoding fileEncoding / iconv() / str_conv()
noticias <- read_csv("http://bit.ly/500-noticias-tecnologia")
head(noticias)
View(noticias)
frecuencias_noticias <- noticias %>%
mutate(documento = 1:nrow(noticias), .before = fecha) %>%
unnest_tokens(input = texto_noticia,
output = palabra) %>%
anti_join(las_stopwords) %>%
count(documento, palabra, sort = TRUE)
frecuencias_noticias %>%
filter(palabra == "henry")
noticias_dtm <- frecuencias_noticias %>%
cast_dtm(documento, palabra, n)
noticias_lda <- LDA(noticias_dtm, k = 6, control = list(seed = 1234))
noticias_tema <- tidy(noticias_lda, matrix = "beta")
noticias_tema %>%
group_by(topic) %>%
slice_max(beta, n = 5) %>%
arrange(topic, -beta) %>%
ggplot(aes(y = term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free")
#https://vas3k.com/blog/machine_learning/
# anaklisis supervissado  https://vas3k.com/blog/machine_learning/https://smltar.com/
#  https://quanteda.io/ quiero entender el concepto de una palabra
titulares_tecnologia %>%
unnest_tokens(input = titular,
output = palabra,
token = "ngrams",
n = 2) %>%
filter(!is.na(palabra)) %>%
count(palabra, sort = TRUE) %>%
separate(col = palabra,
into = c("palabra_1", "palabra_2"),
sep = " ") %>%
filter(!palabra_1 %in% las_stopwords$palabra) %>%
filter(!palabra_2 %in% las_stopwords$palabra)
titulares_tecnologia
titulares_tecnologia %>%
unnest_tokens(input = titular,
output = palabra,
token = "ngrams",
n = 2) %>%
filter(!is.na(palabra)) %>%
count(palabra, sort = TRUE) %>%
separate(col = palabra,
into = c("palabra_1", "palabra_2"),
sep = " ") %>%
filter(!palabra_1 %in% las_stopwords$palabra) %>%
filter(!palabra_2 %in% las_stopwords$palabra)
titulares_tecnologia
# install.packages("tidyverse")
# install.packages("pdftools")
# install.packages("tidytext")
# install.packages("patchwork")
# install.packages("wordcloud")
# install.packages("wordcloud2")
# install.packages("SnowballC")
# install.packages("udpipe")
library(wordcloud)
library(wordcloud2)
library(SnowballC)
library(udpipe) # lematizar (?)
library(tidyverse)
library(pdftools) # me permite leer PDF que tienen OCR o texto que el pdf lo reconoce per se...
library(dplyr)
library(stringr)
library(readr)
library(tidytext)
library(ggplot2)
library(patchwork)
###--- Descargar discurso
download.file("https://obtienearchivo.bcn.cl/obtienearchivo?id=recursoslegales/10221.3/46689/6/20200731.pdf",
destfile = "discurso2020.pdf",
mode = "wb")
# si trabajan en windows, agregar el wb...
# Convertir archivo en objeto tipo caracter..
discurso_2020 <- pdf_text("discurso2020.pdf")
# Unir todas las paginas
discurso_2020 <- paste(discurso_2020, collapse = " ")
# ahora eliminar los \r\n
# abstraer patron y usar expresiones regulares
str_count(discurso_2020, "\r\n                                                                     39\r\n")
# expresiones regulares para los espacios
# forma1 : [:space:]
# forma2 : \\s
# uno o mas elementos: +
# para cantidad especifica (o rango):  ej buscar 14   {14}
# para buscar 2 o mas : {2,}
str_count(discurso_2020, "\r\n[:space:]{2,}39\r\n")
# ahora con digitos
str_count(discurso_2020,"\r\n[:space:]{2,}[:digit:]+\r\n")
# ahi nos aseguramos que tenemos 39 lineas... ahora hay que sacar eso del texto usando la expresion regular
# Eliminar del texto ese patron
discurso_2020 <- str_remove_all(discurso_2020,"\r\n[:space:]{2,}[:digit:]+\r\n")
View(discurso_2020)
## nuevo : Guardar archivo ##
writeLines(discurso_2020,"discurso2020.txt")
# unnest_tokens() nos separa en la medida que le demos
tibble(discurso = discurso_2020) %>%
unnest_tokens(input = discurso, output = palabra)
# output = palabra : me crea la columna palabra y lo deja ahi. la unidad por defecto es la palabra...
# mas info aca : https://www.rdocumentation.org/packages/tidytext/versions/0.2.6/topics/unnest_tokens
# ahora tenemos que sacar la primera parte, para eso hacemos un View y vemos donde empieza el discurso
tibble(discurso = discurso_2020) %>%
unnest_tokens(input = discurso, output = palabra) %>%
View()
# parte en fila 30..
frecuencias_2020 <- tibble(discurso = discurso_2020) %>%
unnest_tokens(input = discurso, output = palabra) %>%
slice(-1:-30) %>%
count(palabra, sort = TRUE)
# me arroja muchisimos articulos, tipo de y la a que en....
# Vamos a sacar los stopwords, que son palabras que sirven para parte gramatical y aparecen mucho. Realmente no me interesan
# ahh riva nos dijo que hay diccionarios con stopwords: palabras vac?as... intedesante. Lo saco de una universidad de Valladolid
# a continuacion se ven 3 formas distintas de sacar palabras:
unas_stopwords <- read.csv("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt
")
mis_stopwords <- tibble(palabra = c('m?s','tenemos','va'))
# vamos a usar un anti join, que es para separar
frecuencias_2020 %>%
anti_join(unas_stopwords) %>%
anti_join(mis_stopwords)
# ahora podemos filtrar por patrones y separar los gentilicios y cosas asi
frecuencias_2020 %>%
anti_join(unas_stopwords) %>%
anti_join(mis_stopwords) %>%
filter(str_detect(palabra,"chile"))
# 1 chile       51
# 2 chilenos    40
# 3 chilenas    14
# 4 chileno      2
# 5 chilena      1
palabras_2020 <- frecuencias_2020 %>%
anti_join(unas_stopwords) %>%
anti_join(mis_stopwords) %>%
filter(!str_detect(palabra,"chile"))
# -----------------------------------------------------------------------------------
# tf_idf  -----------------------------------------------------------------
### Nuevo: Visualizaci?n de palabras (nube de palabras o graficos de barra)
palabras_2020 %>%
slice_head(n=15) %>%
ggplot(aes(y= reorder(palabra,n),n)) +
geom_col(fill = "#0057e7") +
geom_text(aes(label=n)) +
labs(y=NULL,
x="Frecuencia",
title = "Palabras mas frecuentes mensaje 2020",
subtitle = "Presidente Sebasti?n Pi?era",
caption = "Fuente: analisis propio a partir de documento BCN") +
theme_minimal()
# labs deja etiquetas
# mas colores se buscan en color hex las paletas mas usadas  https://www.color-hex.com/
# nube version 1:
wordcloud2(palabras_2020)
# version 2
x11()
wordcloud(words=palabras_2020$palabra,
freq= palabras_2020$n,
min.freq=5,
max.words = 200,
random.order = F,
colors = brewer.pal(6,'Dark2'))
# colors = brewer.pal para ver todas las paletas de colores buscar en  brewer.pal.info (comando de R)
# mas info en nube de palabras: https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html
# vamos a importar otro texto para poder comparar palabras frecuentes y que lo que destaca un documento del otro.
# Palabras 2019
frecuencias_2019 <- read.csv(file="http://bit.ly/frecuencias_2019",fileEncoding="utf-8")
palabras_2019 <- frecuencias_2019 %>%
anti_join(unas_stopwords) %>%
anti_join(mis_stopwords) %>%
filter(!str_detect(palabra,"chile"))
# graficamos..
palabras_2019 %>%
slice_head(n=15) %>%
ggplot(aes(y= reorder(palabra,n),n)) +
geom_col(fill = "#0057e7") +
geom_text(aes(label=n)) +
labs(y=NULL,
x="Frecuencia",
title = "Palabras mas frecuentes mensaje 2019",
subtitle = "Presidente Sebasti?n Pi?era",
caption = "Fuente: analisis propio a partir de documento BCN") +
theme_minimal()
frecuencias_2020 <- frecuencias_2020 %>%
mutate(discurso = 2020, .before = palabra)
frecuencias_2019 <- frecuencias_2019 %>%
mutate(discurso = 2019, .before = palabra)
discursos <- bind_rows(frecuencias_2019, frecuencias_2020)
head(discursos)
tail(discursos)
discursos_tfidf <- bind_tf_idf(discursos,
term = palabra,
document = discurso,
n= n)
head(discursos_tfidf)
tail(discursos_tfidf)
x11()
discursos_tfidf %>%
arrange(desc(tf_idf)) %>%
group_by(discurso) %>%
slice_head(n=15) %>%
ggplot(aes(y=reorder(palabra,tf_idf),x= tf_idf, fill= discurso)) +
geom_col(show.legend = FALSE) +
facet_wrap(~discurso, scales = "free") +
labs(y=NULL) +
scale_x_continuous(labels = scales::comma)
# usaremos los paquetes de arriba para lematizar: no perder la categor?a gramatical, si no quedarnos con ...
# Vamos a hacer etiquetado sint?ctico del discurso.
udpipe_download_model(language = "spanish") # bajamos el paquete espa?ol
modelo_es <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
# ya tengo el modelo cargado, ahora lo tenemos que usar.
discurso_2020_anotado <- udpipe_annotate(modelo_es,
x=discurso_2020)
# ojo de usar discurso  y NO dataframe
# ahora que ya hizo el annotate, lo pasamos a dataframe para an?lisis.
discurso_2020_anotado <- as_tibble(discurso_2020_anotado)
head(discurso_2020_anotado)
View(discurso_2020_anotado)
# verbos mas buscados
discurso_2020_anotado %>%
slice(-1:-36) %>%
filter(upos=="VERB") %>%
count(lemma, sort = TRUE )
# adjetivos mas buscados
discurso_2020_anotado %>%
slice(-1:-36) %>%
filter(upos=="ADJ") %>%
count(lemma, sort = TRUE )
palabras_2019 %>%
slice_head(n=15) %>%
ggplot(aes(y= reorder(palabra,n),n)) +
geom_col(fill = "#0057e7") +
geom_text(aes(label=n)) +
labs(y=NULL,
x="Frecuencia",
title = "Palabras mas frecuentes mensaje 2019",
subtitle = "Presidente Sebasti?n Pi?era",
caption = "Fuente: analisis propio a partir de documento BCN") +
theme_minimal()
.before
frecuencias_2020 <- frecuencias_2020 %>%
mutate(discurso = 2020, .before = palabra)
frecuencias_2020
discursos <- bind_rows(frecuencias_2019, frecuencias_2020)
head(discursos)
tail(discursos)
discursos_tfidf <- bind_tf_idf(discursos,
term = palabra,
document = discurso,
n= n)
head(discursos_tfidf)
tail(discursos_tfidf)
?tail
discursos_tfidf <- bind_tf_idf(discursos,
term = palabra,
document = discurso,
n= n)
head(discursos_tfidf)
?tail
tail(discursos_tfidf)
x11()
discursos_tfidf %>%
arrange(desc(tf_idf)) %>%
group_by(discurso) %>%
slice_head(n=15) %>%
ggplot(aes(y=reorder(palabra,tf_idf),x= tf_idf, fill= discurso)) +
geom_col(show.legend = FALSE) +
facet_wrap(~discurso, scales = "free") +
labs(y=NULL) +
scale_x_continuous(labels = scales::comma)
head(discursos_tfidf)
x11()
discursos_tfidf %>%
arrange(desc(tf_idf)) %>%
group_by(discurso) %>%
slice_head(n=15) %>%
ggplot(aes(y=reorder(palabra,tf_idf),x= tf_idf, fill= discurso)) +
geom_col(show.legend = FALSE) +
facet_wrap(~discurso, scales = "free") +
labs(y=NULL) +
scale_x_continuous(labels = scales::comma)
discursos_tfidf <- bind_tf_idf(discursos,
term = palabra,
document = discurso,
n= n)
discursos_tfidf
library(rvest)
library(robotstxt)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(readr)
library(tidytext)
library(topicmodels)
library(ggplot2)
get_robotstxt("https://www2.animeflv.to/")
enlace_p1 <- read_html("https://www2.animeflv.to/")
texto_titulares <- enlace_p1 %>%
html_nodes("strong.Title") %>%  # seleciono solos los strong con la class title
html_text(trim = TRUE)
enlace_titulares <- enlace_p1 %>%
html_nodes("li > a.fa-play") %>%
html_attr("href")
#enlace_titulares <- str_remove_all(enlace_titulares,"[:digit:]")
enlace_titulares <- str_replace_all(enlace_titulares, "[:digit:]", "")
enlace_titulares <-  substr(enlace_titulares, start = 0, stop = nchar(enlace_titulares) - 1 )
enlace_titulares <-   gsub("ver/", "anime/", enlace_titulares)
tibble(titular = texto_titulares,
enlace_anime =  gsub("ver/", "anime/", enlace_titulares)  ) %>%
mutate(enlace_anime = paste0("https://www2.animeflv.to", enlace_anime))
# separate(col = titular,
#          into = c("Titulo", "Descripcion"),
#          sep = "  ",
#          extra = "merge")
# texto_titulares <- html %>%
#   html_nodes("strong.Title") %>%  # seleciono solos los strong con la class title
#   html_text(trim = TRUE)
#
#
# enlace_titulares <- html %>%
#   html_nodes("li > a.fa-play") %>%
#   html_attr("href")
#
#
# #enlace_titulares <- str_remove_all(enlace_titulares,"[:digit:]")
#
# enlace_titulares <- str_replace_all(enlace_titulares, "[:digit:]", "")
#
# enlace_titulares <-  substr(enlace_titulares, start = 0, stop = nchar(enlace_titulares) - 1 )
#
# tibble(titular = texto_titulares,
#        enlace_anime =  gsub("ver/", "anime/", enlace_titulares)  ) %>%
#   mutate(enlace_anime = paste0("https://www2.animeflv.to", enlace_anime))
}
obtener_titulares(enlace_titulares)
